# Linear Regression cost 최소화의 원리

### 경사하강법

> 비용 최소화 알고리즘의 가장 대표적인 유형

- y  = WX + b

  - b는 생략하고 WX가 예측값이라고 가정할 때
    - X = 주어지는 입력값
    - 결국,  W (1차원 방정식 그래프의 기울기) 의 값에 따라 cost 의 최소값이 정해짐

- W, b

  - 랜덤한 초기값으로 시작
  - w, b 값을 미소하게 조정하면서 cost(W)의 최소값 구하기 (2차원 방정식 그래프의 최소값 구하기)

- 미분 수식 구하는 사이트

  - https://www.derivative-calculator.net/

- 루프를 돌면서 새로 조정되는 W 값을 다시 cost(W) 수식에 대입하여 그 값이 최소가 되는 지점 구하게 됨

- convex function

  - Linear Regression을 적용하기 전에, convex function의 그래프 형태가 맞는지 확인해야 함

  - convex function의 경우, 어떤 지점에서 시작하더라도 2차원 그래프의 형태를 띠게 되므로 적용 가능

  - 그 외, 비정형 형태의 그래프인 경우 Linear Regression 적용이 다소 어려움

    - 경사하강법 적용 시, cost(W) 값이 하강하는 대신 증가할 수도 있으므로

    ![image-20191202221909084](C:\Users\박진영\AppData\Roaming\Typora\typora-user-images\image-20191202221909084.png)

- sess.run 최적화 순서

  - train
    - 통상 minimize 역할을 수행하는 노드
    - 루프 안에서 별도 작성해주는 것이 좋음
      - sess.run(train)
  - cost
    - minimize 가 루프를 돌면서 실행될 때마다 값이 조정됨
  - hypothesis
    - minimize 가 루프를 돌면서 실행될 때마다 값이 조정됨
  - W, b
    - minimize 가 루프를 돌면서 실행될 때마다 값이 조정됨